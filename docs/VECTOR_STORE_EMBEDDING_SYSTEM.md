# Vector Store & Embedding System

## Overview

DeepTalk implements a sophisticated **Retrieval-Augmented Generation (RAG) system** for semantic search and AI-powered chat functionality. The system is designed as a privacy-first, locally-running solution using embedded vector databases and local language models, providing complete user data control without external API dependencies.

## System Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    React Frontend (Renderer)               │
├─────────────────────────────────────────────────────────────┤
│  Service Layer (TypeScript)                                │
│  ├── EmbeddingService (🔗 Main Process)                    │
│  ├── VectorStoreService (🔗 Main Process)                  │
│  ├── ChunkingService (Local)                               │
│  ├── ChatService (Orchestration)                           │
│  └── ProjectChatService (Cross-transcript)                 │
├─────────────────────────────────────────────────────────────┤
│  Main Process (Electron)                                   │
│  ├── LanceDB (Vector Storage)                              │
│  ├── @xenova/transformers (Embeddings)                     │
│  └── IPC Bridges                                           │
├─────────────────────────────────────────────────────────────┤
│  Storage & External Services                               │
│  ├── SQLite (Metadata & Chat History)                      │
│  ├── Local Vector Database                                 │
│  └── Ollama (LLM Generation)                               │
└─────────────────────────────────────────────────────────────┘
```

## Core Components

### 1. Embedding Service (`/src/services/embeddingService.ts`)

**Purpose**: Generates vector embeddings using local transformer models for privacy-first semantic understanding.

#### Model Configuration
```typescript
interface EmbeddingConfig {
  model: 'Xenova/all-MiniLM-L6-v2';  // ~25MB model
  maxLength: 512;                    // Token limit
  normalize: true;                   // Vector normalization
  dimensions: 384;                   // Output vector dimensions
}
```

#### Key Features
- **Local Processing**: Browser-compatible execution via `@xenova/transformers`
- **IPC Architecture**: Delegates to main process for native dependencies
- **Singleton Pattern**: Memory-efficient single instance
- **Batch Processing**: Support for multiple texts simultaneously
- **Model Caching**: Single model instance with automatic cleanup

#### Performance Characteristics
- **Model Size**: ~25MB (quick download/loading)
- **Dimensions**: 384 (optimal balance of quality vs. performance)
- **Processing Speed**: ~100 chunks/minute on average hardware
- **Memory Usage**: ~50MB base footprint

#### Implementation Highlights
```typescript
class EmbeddingService {
  private static instance: EmbeddingService;
  private model: any = null;
  
  async generateEmbedding(text: string): Promise<number[]> {
    // Delegate to main process for native dependencies
    return await window.electronAPI.generateEmbedding(text);
  }
  
  async generateEmbeddings(texts: string[]): Promise<number[][]> {
    // Batch processing for efficiency
    return await window.electronAPI.generateEmbeddings(texts);
  }
}
```

### 2. Vector Store Service (`/src/services/vectorStoreService.ts`)

**Purpose**: Manages vector storage and similarity search using LanceDB embedded database.

#### Vector Schema
```typescript
interface VectorChunk {
  id: string;                // Unique identifier
  transcriptId: string;      // Source transcript
  text: string;             // Chunk content
  vector: number[];         // 384-dimensional embedding
  startTime: number;        // Temporal position (seconds)
  endTime: number;          // Temporal end (seconds)
  speaker?: string;         // Speaker attribution
  chunkIndex: number;       // Sequence position
  wordCount: number;        // Size metric for relevance
  speakers: string[];       // All speakers in chunk
  method: string;           // Chunking method used
  createdAt: string;        // Creation timestamp
}
```

#### Search Capabilities
```typescript
interface SearchOptions {
  limit?: number;           // Result count (default: 5)
  minScore?: number;        // Similarity threshold (0-1)
  transcriptId?: string;    // Filter by source transcript
  speaker?: string;         // Filter by speaker
  timeRange?: {             // Temporal filtering
    start: number; 
    end: number;
  };
}
```

#### Key Features
- **LanceDB Integration**: High-performance embedded vector database
- **Rich Metadata**: Comprehensive chunk information storage
- **Advanced Filtering**: Time range, speaker, transcript filters
- **Similarity Search**: Cosine similarity with configurable thresholds
- **Result Caching**: 5-minute cache for repeated queries
- **IPC Delegation**: Main process execution for native dependencies

#### Performance Optimization
- **Index Management**: Automatic indexing for fast retrieval
- **Filtering Efficiency**: Pre-filtering reduces search space
- **Memory Management**: Efficient vector storage and retrieval
- **Search Latency**: <100ms for similarity search with 1000+ chunks

### 3. Chunking Service (`/src/services/chunkingService.ts`)

**Purpose**: Intelligent text segmentation for optimal embedding and retrieval performance.

#### Chunking Strategies

##### 1. Speaker-Based Chunking (Recommended)
```typescript
interface SpeakerChunkingConfig {
  method: 'speaker';
  maxChunkSize: 60;        // seconds
  minChunkSize: 5;         // minimum viable chunk
  contextOverlap: 10;      // seconds overlap
}
```

**Features**:
- Splits on speaker turn changes
- Maintains conversational context
- Fallback to time-based for oversized chunks
- Preserves dialogue boundaries

##### 2. Time-Based Chunking
```typescript
interface TimeChunkingConfig {
  method: 'time';
  chunkDuration: 30;       // fixed duration chunks
  chunkOverlap: 10;        // overlap for context preservation
}
```

**Features**:
- Fixed duration with configurable overlap
- Consistent processing for media files
- 10-second overlap prevents context loss
- Suitable for monologue content

##### 3. Hybrid Chunking
```typescript
interface HybridChunkingConfig {
  method: 'hybrid';
  maxChunkSize: 60;        // time constraint
  speakerBoundary: true;   // respect speaker changes
  adaptiveSize: true;      // dynamic boundaries
}
```

**Features**:
- Speaker-based with time constraints
- Automatically splits large speaker segments
- Best balance of context and performance
- Adaptive sizing based on content

#### Quality Metrics
- **Context Preservation**: 95%+ context retention with overlap strategies
- **Chunk Statistics**: Word count, speaker analysis, temporal metrics
- **Adaptive Sizing**: Dynamic boundaries based on content structure
- **Boundary Detection**: Intelligent sentence and speaker boundary respect

### 4. Chat Service (`/src/services/chatService.ts`)

**Purpose**: Orchestrates the complete RAG pipeline and manages conversation state.

#### Conversation Modes

##### 1. RAG Mode (Default)
**Flow**: Vector Search → Context Building → LLM Generation

```typescript
async processRAGQuery(query: string, transcriptId: string): Promise<ChatResponse> {
  // 1. Generate query embedding
  const queryEmbedding = await embeddingService.generateEmbedding(query);
  
  // 2. Perform similarity search
  const relevantChunks = await vectorStoreService.searchSimilar(
    queryEmbedding, 
    { transcriptId, limit: 5, minScore: 0.3 }
  );
  
  // 3. Build context from chunks
  const context = this.buildContextFromChunks(relevantChunks);
  
  // 4. Generate response with LLM
  return await this.generateResponse(query, context);
}
```

##### 2. Vector-Only Mode
**Purpose**: Direct chunk retrieval without LLM interpretation

```typescript
async processVectorOnlyQuery(query: string): Promise<VectorSearchResult[]> {
  const queryEmbedding = await embeddingService.generateEmbedding(query);
  return await vectorStoreService.searchSimilar(queryEmbedding, options);
}
```

##### 3. Direct-LLM Mode
**Purpose**: Full transcript context to LLM (within limits)

```typescript
async processDirectLLMQuery(query: string, transcript: string): Promise<ChatResponse> {
  // Smart truncation with dynamic context management
  const truncatedTranscript = this.smartTruncate(transcript);
  return await this.generateResponse(query, truncatedTranscript);
}
```

#### Memory Management
```typescript
interface ConversationMemory {
  activeMessages: ChatMessage[];     // Recent messages
  compactedSummary: string;         // Summarized history
  totalExchanges: number;           // Conversation length
  lastCompactionAt?: string;        // Compaction timestamp
}
```

#### Dynamic Context Management
- **Model Metadata Integration**: Detects model capabilities and context limits
- **Adaptive Budgeting**: Allocates context between content and memory
- **Smart Truncation**: Prioritizes recent conversations over older content
- **Memory Reserve**: 20% of context reserved for conversation history

### 5. Project Chat Service (`/src/services/projectChatService.ts`)

**Purpose**: Cross-transcript analysis and project-level semantic search.

#### Analysis Modes

##### 1. Collated Analysis
```typescript
async collatedAnalysis(query: string, transcriptIds: string[]): Promise<ProjectChatResponse> {
  const results = await Promise.all(
    transcriptIds.map(id => this.searchTranscript(query, id))
  );
  return this.mergeResults(results);
}
```

##### 2. Cross-Transcript Analysis
```typescript
async crossTranscriptAnalysis(query: string): Promise<ProjectChatResponse> {
  // Search across all transcripts simultaneously
  const allChunks = await vectorStoreService.searchSimilar(queryEmbedding, {
    limit: 20, // Larger result set for analysis
    minScore: 0.2
  });
  
  // Analyze patterns across sources
  return this.analyzePatterns(allChunks, query);
}
```

##### 3. Hybrid Analysis
**Features**:
- Question type analysis (specific vs. thematic)
- Dynamic strategy selection based on content
- Combined approaches for comprehensive answers

#### Content Selection Strategies
- **Relevant**: Vector similarity-based transcript selection
- **Recent**: Time-based selection for temporal analysis
- **All**: Comprehensive analysis across all transcripts

### 6. Search Implementation

#### Cross-Transcript Search (`/src/components/ProjectCrossTranscriptSearch.tsx`)
```typescript
interface SearchFeatures {
  fieldSpecificSearch: string[];    // text, themes, quotes, insights
  advancedFiltering: {
    dateRange: DateRange;
    speakers: string[];
    sentiment: SentimentFilter;
  };
  relevanceScoring: RelevanceAlgorithm;
  contextHighlighting: boolean;
  realTimeSearch: boolean;          // with debouncing
}
```

#### Global Search (`/src/pages/SearchPage.tsx`)
**Features**:
- Unified search across transcripts and projects
- Bulk operations for project management
- Multi-select capabilities
- Advanced filtering by duration, sentiment, keywords

#### Search Algorithm
```typescript
interface SearchResult {
  content: string;
  relevanceScore: number;
  context: string;              // 100-character windows
  source: TranscriptMetadata;
  highlights: TextHighlight[];
  temporalBoost: number;        // Recent content boost
}
```

## Data Flow Architecture

### Document Processing Pipeline
```
Raw Transcript → Chunking Strategy Selection → Text Segmentation → 
Embedding Generation → Vector Storage → Metadata Indexing
```

### Query Processing Pipeline
```
User Query → Query Embedding → Vector Similarity Search → 
Context Building → Memory Integration → LLM Generation → Response
```

### Cross-Transcript Analysis Pipeline
```
Project Query → Transcript Selection Strategy → Multi-Source Vector Search → 
Pattern Analysis → Theme Evolution → Synthesis Response
```

## Integration Architecture

### 1. Electron Integration
```typescript
// Preload script exposure
contextBridge.exposeInMainWorld('electronAPI', {
  generateEmbedding: (text: string) => ipcRenderer.invoke('generate-embedding', text),
  searchVectors: (embedding: number[], options: SearchOptions) => 
    ipcRenderer.invoke('search-vectors', embedding, options),
  storeVectors: (chunks: VectorChunk[]) => 
    ipcRenderer.invoke('store-vectors', chunks)
});
```

**Features**:
- **Secure IPC**: Controlled API exposure to renderer
- **Resource Management**: Automatic cleanup and memory management
- **Process Isolation**: Native dependencies in main process
- **Error Boundaries**: Graceful failure handling

### 2. Database Integration

#### SQLite Integration
```sql
-- Chat history and conversation memory
CREATE TABLE chat_conversations (
    id TEXT PRIMARY KEY,
    transcript_id TEXT,
    messages TEXT, -- JSON array
    memory TEXT,   -- Compacted conversation summary
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

#### LanceDB Integration
```typescript
// Vector database schema
interface LanceDBTable {
  vectors: Float32Array[];      // 384-dimensional embeddings
  metadata: VectorChunk[];      // Rich chunk metadata
  indexes: SpatialIndex[];      // Optimized search indexes
}
```

### 3. External Service Integration

#### Ollama Integration
```typescript
interface OllamaConfig {
  baseURL: string;              // Local Ollama instance
  model: string;               // Selected LLM model
  contextLimit: number;        // Model-specific limits
  streamResponse: boolean;     // Streaming responses
}
```

**Features**:
- **Local LLM**: Privacy-first response generation
- **Model Detection**: Automatic capability detection
- **Streaming**: Real-time response streaming
- **Fallback**: Graceful degradation when unavailable

## Performance Characteristics

### Scalability Metrics
- **Embedding Speed**: ~100 chunks/minute on average hardware
- **Search Latency**: <100ms for similarity search with 1000+ chunks
- **Memory Usage**: ~50MB base + ~1KB per chunk stored
- **Storage Efficiency**: ~400 bytes per vector chunk + metadata

### Quality Metrics
- **Embedding Quality**: all-MiniLM-L6-v2 provides excellent semantic understanding
- **Chunking Quality**: 95%+ context preservation with speaker-based chunking
- **Search Relevance**: Multi-factor scoring provides high precision
- **Response Quality**: RAG provides contextually accurate responses

### Optimization Strategies

#### 1. Embedding Optimization
```typescript
class EmbeddingOptimizer {
  private modelCache: Map<string, any> = new Map();
  private batchQueue: string[] = [];
  
  async optimizedGeneration(texts: string[]): Promise<number[][]> {
    // Batch processing for efficiency
    const batches = this.createBatches(texts, 32);
    return Promise.all(batches.map(batch => this.processBatch(batch)));
  }
}
```

#### 2. Vector Search Optimization
```typescript
interface SearchOptimization {
  indexingStrategy: 'HNSW' | 'IVF' | 'Flat';
  cacheStrategy: LRUCache<string, SearchResult[]>;
  prefiltering: boolean;           // Filter before similarity computation
  approximateSearch: boolean;      // Trade accuracy for speed
}
```

#### 3. Context Management Optimization
```typescript
interface ContextBudget {
  totalLimit: number;              // Model context limit
  memoryReserve: number;          // 20% for conversation history
  contentBudget: number;          // Available for retrieval content
  safetyMargin: number;           // 10% buffer
  estimatedTokens: number;        // Characters to tokens estimation
}
```

## Advanced Features

### 1. Multi-Modal Conversation Management
```typescript
interface ConversationMode {
  vectorOnly: {
    purpose: 'fact-finding';
    output: 'raw-chunks';
    llm: false;
  };
  rag: {
    purpose: 'contextual-chat';
    output: 'generated-response';
    llm: true;
    retrieval: true;
  };
  directLLM: {
    purpose: 'simple-queries';
    output: 'generated-response';
    llm: true;
    retrieval: false;
  };
}
```

### 2. Cross-Transcript Intelligence
```typescript
interface CrossTranscriptAnalysis {
  patternRecognition: {
    themeEvolution: TemporalPattern[];
    consensusPoints: Agreement[];
    divergencePoints: Disagreement[];
  };
  temporalAnalysis: {
    trendIdentification: Trend[];
    cyclicalPatterns: Pattern[];
    anomalyDetection: Anomaly[];
  };
}
```

### 3. Privacy-First Design
```typescript
interface PrivacyFeatures {
  localProcessing: boolean;        // All embeddings generated locally
  noExternalCalls: boolean;       // Vector storage entirely local
  userDataControl: boolean;       // Complete data ownership
  encryptionAtRest: boolean;      // Optional data encryption
}
```

## Key Technical Innovations

### 1. Intelligent Chunking Algorithm
```typescript
class SmartChunker {
  selectStrategy(content: TranscriptContent): ChunkingStrategy {
    if (content.hasSpeakers && content.speakers.length > 1) {
      return 'speaker-based';
    } else if (content.hasTimestamps) {
      return 'time-based';
    } else {
      return 'hybrid';
    }
  }
}
```

### 2. Dynamic Context Allocation
```typescript
class ContextManager {
  calculateOptimalAllocation(
    modelLimits: ModelMetadata,
    contentSize: number,
    memorySize: number
  ): AllocationStrategy {
    // Intelligent context budgeting based on content and memory
    return this.optimizeAllocation(modelLimits, contentSize, memorySize);
  }
}
```

### 3. Multi-Source Pattern Analysis
```typescript
class PatternAnalyzer {
  async analyzeAcrossTranscripts(
    query: string,
    transcripts: TranscriptChunk[][]
  ): Promise<PatternAnalysis> {
    // Advanced pattern recognition across multiple sources
    return this.identifyPatterns(query, transcripts);
  }
}
```

## Strengths and Capabilities

1. **Complete RAG Implementation**: Full pipeline from chunking to response generation
2. **Privacy-First Architecture**: Complete local processing with no external dependencies
3. **Multi-Modal Flexibility**: Multiple conversation modes for different use cases
4. **Intelligent Chunking**: Context-aware segmentation strategies
5. **Cross-Transcript Analysis**: Advanced pattern recognition across multiple sources
6. **Dynamic Context Management**: Model-aware resource allocation
7. **Production-Ready**: Robust error handling, caching, and optimization
8. **Extensible Design**: Modular architecture for easy enhancement

## Reuse Value for Other Applications

This Vector Store & Embedding System provides a comprehensive foundation for any application requiring:

- **Semantic Search Capabilities** with local embedding generation
- **Privacy-First RAG Implementation** without external API dependencies
- **Cross-Document Analysis** with pattern recognition
- **Intelligent Text Chunking** with multiple strategies
- **Conversation Memory Management** with context optimization
- **Multi-Modal Chat Interfaces** with flexible interaction modes
- **Performance-Optimized Vector Search** with caching and indexing
- **Scalable Architecture** for large document collections

The system represents a state-of-the-art implementation of local RAG capabilities, providing sophisticated semantic search and AI chat functionality while maintaining complete user privacy and data control.